<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/pr-preview/pr-2/favicon.ico"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v4.6.2"><title>OSSI@Janelia</title><style is:global>
  :root {
    --font-sans: "Inter Variable";
    --font-serif: var(--font-sans);
    --font-heading: var(--font-sans);

    /* --color-primary: rgb(30 64 175); */
    --color-primary: rgb(5 141 150);
    /* --color-secondary: rgb(30 58 138); */
    --color-secondary: rgb(0 164 80);
    /* --color-accent: rgb(109 40 217); */
    --color-accent: rgb(63 194 205);

    --color-text-heading: rgb(0 0 0);
    --color-text-default: rgb(16 16 16);
    --color-text-muted: rgb(16 16 16 / 66%);

    --color-bg-page: rgb(255 255 255);
    --color-bg-page-dark: rgb(3 6 32);

    /* default color - this is dynamically updated by handleTagSelection */
    --color-bg-selected-tag: var(--color-primary);
  }

  .dark {
    --font-sans: "Inter Variable";
    --font-serif: var(--font-sans);
    --font-heading: var(--font-sans);

    /* --color-primary: rgb(30 64 175); */
    --color-primary: rgb(5 141 150);
    /* --color-secondary: rgb(30 58 138); */
    --color-secondary: rgb(0 164 80);
    /* --color-accent: rgb(109 40 217); */
    --color-accent: rgb(63 194 205);

    --color-text-heading: rgb(0 0 0);
    --color-text-default: rgb(229 236 246);
    --color-text-muted: rgb(229 236 246 / 66%);

    --color-bg-page: var(--color-bg-page-dark);

    /* default color - this is dynamically updated by handleTagSelection */
    --color-bg-selected-tag: var(--color-primary);
  }
</style><link rel="stylesheet" href="/pr-preview/pr-2/_astro/_slug_.DPN5iaBe.css">
<style>.twitter-tweet:not(.twitter-tweet-rendered){padding:var(--tc-padding, 1em);border:1px solid var(--tc-border-color, #cfd9de)}.twitter-tweet:not(.twitter-tweet-rendered)>:first-child{margin-top:0}.twitter-tweet:not(.twitter-tweet-rendered)>:last-child{margin-bottom:0}lite-youtube{background-color:#000;position:relative;display:block;contain:content;background-position:center center;background-size:cover;cursor:pointer;max-width:720px}lite-youtube:before{content:attr(data-title);display:block;position:absolute;top:0;background-image:linear-gradient(180deg,#000000ab,#0000008a 14%,#00000026 54%,#0000000d 72%,#0000 94%);height:99px;width:100%;font-family:YouTube Noto,Roboto,Arial,Helvetica,sans-serif;color:#eee;text-shadow:0 0 2px rgba(0,0,0,.5);font-size:18px;padding:25px 20px;overflow:hidden;white-space:nowrap;text-overflow:ellipsis;box-sizing:border-box}lite-youtube:hover:before{color:#fff}lite-youtube:after{content:"";display:block;padding-bottom:56.25%}lite-youtube>iframe{width:100%;height:100%;position:absolute;top:0;left:0;border:0}lite-youtube>.lty-playbtn{display:block;width:100%;height:100%;background:no-repeat center/68px 48px;background-image:url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 68 48"><path d="M66.52 7.74c-.78-2.93-2.49-5.41-5.42-6.19C55.79.13 34 0 34 0S12.21.13 6.9 1.55c-2.93.78-4.63 3.26-5.42 6.19C.06 13.05 0 24 0 24s.06 10.95 1.48 16.26c.78 2.93 2.49 5.41 5.42 6.19C12.21 47.87 34 48 34 48s21.79-.13 27.1-1.55c2.93-.78 4.64-3.26 5.42-6.19C67.94 34.95 68 24 68 24s-.06-10.95-1.48-16.26z" fill="red"/><path d="M45 24 27 14v20" fill="white"/></svg>');position:absolute;cursor:pointer;z-index:1;filter:grayscale(100%);transition:filter .1s cubic-bezier(0,0,.2,1);border:0}lite-youtube:hover>.lty-playbtn,lite-youtube .lty-playbtn:focus{filter:none}lite-youtube.lyt-activated{cursor:unset}lite-youtube.lyt-activated:before,lite-youtube.lyt-activated>.lty-playbtn{opacity:0;pointer-events:none}.lyt-visually-hidden{clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px}lite-youtube>iframe{all:unset!important;width:100%!important;height:100%!important;position:absolute!important;inset:0!important;border:0!important}lite-vimeo{font-size:10px;background-color:#000;position:relative;display:block;contain:content;background-position:center center;background-size:cover}lite-vimeo:after{content:"";display:block;padding-bottom:56.25%}lite-vimeo>iframe{all:unset!important;width:100%!important;height:100%!important;position:absolute!important;inset:0!important;border:0!important}lite-vimeo>.ltv-playbtn{content:"";position:absolute;inset:0;width:100%;background:transparent;outline:0;border:0;cursor:pointer}lite-vimeo>.ltv-playbtn:before{width:6.5em;height:4em;background:#172322bf;opacity:.8;border-radius:.25rem;transition:all .2s cubic-bezier(0,0,.2,1)}lite-vimeo>.ltv-playbtn:focus:before{outline:auto}lite-vimeo:hover>.ltv-playbtn:before{background-color:#00adef;background-color:var(--ltv-color, #00adef);opacity:1}lite-vimeo>.ltv-playbtn:after{border-style:solid;border-width:1em 0 1em 1.7em;border-color:transparent transparent transparent #fff}lite-vimeo>.ltv-playbtn:before,lite-vimeo>.ltv-playbtn:after{content:"";position:absolute;top:50%;left:50%;transform:translate3d(-50%,-50%,0)}lite-vimeo.ltv-activated:before,lite-vimeo.ltv-activated>.ltv-playbtn{cursor:unset;opacity:0;pointer-events:none}
</style><script type="module" src="/pr-preview/pr-2/_astro/hoisted.XVAdaamv.js"></script></head> <body class="text-default bg-page"> <header class="scroll fixed top-0 z-40 flex-none mx-auto w-full border-b border-gray-50/0 transition-[opacity] ease-in-out" data-sticky-header="true" id="header"> <div class="absolute inset-0 bg-white dark:bg-dark"></div> <div class="relative text-default p-3 md:px-6 w-full flex justify-between"> <a class="flex items-center" href="/pr-preview/pr-2/tmp/pr-preview/pr-2"> <span class="self-center ml-2 rtl:ml-0 rtl:mr-2 text-2xl font-bold text-gray-900 whitespace-nowrap dark:text-white"></span> <span class="min-[600px]:hidden self-center ml-2 rtl:ml-0 rtl:mr-2 text-2xl lg:text-xl font-bold text-gray-900 whitespace-nowrap dark:text-white"> ðŸ”¬   Janelia OSSI </span> <span class="hidden min-[600px]:block self-center ml-2 rtl:ml-0 rtl:mr-2 text-2xl lg:text-xl font-bold text-gray-900 whitespace-nowrap dark:text-white"> ðŸ”¬   Janelia Open Source Science Initiative </span> </a> <div class="flex"> <div class="flex items-center lg:hidden order-2"> <button class="flex flex-col h-12 w-12 rounded justify-center items-center cursor-pointer group" aria-label="Toggle Menu" data-aw-toggle-menu> <span class="sr-only">Toggle Menu</span>  <span aria-hidden="true" class="h-0.5 w-6 my-1 rounded-full bg-black dark:bg-white transition ease transform duration-200 opacity-80 group-[.expanded]:rotate-45 group-[.expanded]:translate-y-2.5"></span> <span aria-hidden="true" class="h-0.5 w-6 my-1 rounded-full bg-black dark:bg-white transition ease transform duration-200 opacity-80 group-[.expanded]:opacity-0"></span> <span aria-hidden="true" class="h-0.5 w-6 my-1 rounded-full bg-black dark:bg-white transition ease transform duration-200 opacity-80 group-[.expanded]:-rotate-45 group-[.expanded]:-translate-y-2.5"></span>  </button> </div> <nav class="items-center w-full lg:w-auto hidden lg:flex text-default overflow-y-auto overflow-x-hidden lg:overflow-y-visible lg:overflow-x-auto lg:mx-5" aria-label="Main navigation"> <ul class="flex flex-col md:flex-row md:self-center w-full md:w-auto text-xl md:text-[0.9375rem] tracking-[0.01rem] font-medium"> <li class=""> <a class="hover:text-link dark:hover:text-white px-4 py-3 flex items-centers" href="/pr-preview/pr-2"> Home </a> </li><li class=""> <a class="hover:text-link dark:hover:text-white px-4 py-3 flex items-centers" href="/pr-preview/pr-2/tmp/pr-preview/pr-2/projects"> Projects </a> </li><li class=""> <a class="hover:text-link dark:hover:text-white px-4 py-3 flex items-centers" href="/pr-preview/pr-2/tmp/pr-preview/pr-2/ecosystems"> Ecosystems </a> </li><li class=""> <a class="hover:text-link dark:hover:text-white px-4 py-3 flex items-centers" href="/pr-preview/pr-2/tmp/pr-preview/pr-2/ossi"> OSSI </a> </li><li class=""> <a class="hover:text-link dark:hover:text-white px-4 py-3 flex items-centers" href="/pr-preview/pr-2/tmp/pr-preview/pr-2/blog"> Blog </a> </li> </ul> </nav> <div id="toggle-theme" class="self-center flex items-center w-auto static justify-end"> <button type="button" class="text-muted dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm p-2.5 inline-flex items-center" aria-label="Toggle between Dark and Light mode" data-toggle-color-scheme> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-6 h-6" data-icon="tabler:sun">  <symbol id="ai:tabler:sun"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 12a4 4 0 1 0 8 0a4 4 0 1 0-8 0m-5 0h1m8-9v1m8 8h1m-9 8v1M5.6 5.6l.7.7m12.1-.7l-.7.7m0 11.4l.7.7m-12.1-.7l-.7.7"/></symbol><use xlink:href="#ai:tabler:sun"></use>  </svg> </button> </div> </div> </div> </header>  <div class="pt-24 pb-8 max-w-7xl mx-auto"> <a class="self-start mx-6 border-b-2 text-sm transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="/pr-preview/pr-2/projects">
Projects
</a> <div class="flex flex-col lg:grid grid-cols-6"> <section class="row-start-1 row-span-1 col-span-4 pt-6 pb-8"> <h1 class="mx-6 text-4xl md:text-5xl font-bold leading-tighter tracking-tighter font-heading"> neuVid </h1> <p class="mx-6 mt-6 md:mt-10 text-xl md:text-2xl text-muted dark:text-slate-400"> Make the neuVid 3D visualization software more accessible to the community. </p> </section> <div class="px-6 py-12 lg:p-0 lg:pl-12 row-start-1 row-span-2 col-start-5 col-span-2 bg-primary lg:bg-page dark:bg-slate-900"> <div class="p-6 rounded-md shadow-sm lg:shadow-none bg-page"> <h2 class="text-xl font-bold mb-6">Quick links</h2> <div class="flex flex-row flex-wrap gap-6 mx-auto"> <div class="flex flex-col gap-1 mr-8"> <h3 class="font-bold mb-1">Get started</h3> <a class="self-start flex items-center gap-1 transition ease-in duration-200 hover:bg-gray-100 dark:text-slate-300 dark:hover:bg-slate-800 cursor-pointer" href="https://github.com/connectome-neuprint/neuVid"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5  inline-block" data-icon="tabler:square-chevron-right">  <symbol id="ai:tabler:square-chevron-right"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m11 9l3 3l-3 3"/><path d="M3 5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/></g></symbol><use xlink:href="#ai:tabler:square-chevron-right"></use>  </svg>
Source code
</a> <a class="self-start flex items-center gap-1 transition ease-in duration-200 hover:bg-gray-100 dark:text-slate-300 dark:hover:bg-slate-800 cursor-pointer" href="https://github.com/connectome-neuprint/neuVid"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5  inline-block" data-icon="tabler:square-chevron-right">  <use xlink:href="#ai:tabler:square-chevron-right"></use>  </svg>
Documentation
</a> <a class="self-start flex items-center gap-1 transition ease-in duration-200 hover:bg-gray-100 dark:text-slate-300 dark:hover:bg-slate-800 cursor-pointer" href="https://github.com/connectome-neuprint/neuVid"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5  inline-block" data-icon="tabler:square-chevron-right">  <use xlink:href="#ai:tabler:square-chevron-right"></use>  </svg>
How to install
</a> </div> <div class="flex flex-col gap-1 mr-8 md:mr-12"> <h3 class="font-bold mb-1">Get help</h3> <a class="self-start btn rounded-md p-2 font-normal" href="https://github.com/connectome-neuprint/neuVid">
Open an issue on GitHub
</a> <a class="self-start btn rounded-md p-2 font-normal" href="placeholder@gmail.com">
Reach out with a question
</a> </div> <div class="flex flex-col gap-1"> <h3 class="font-bold mb-1">Cite this software</h3> <a class=" self-start border-b-2 transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="https://github.com/connectome-neuprint/neuVid">Cite the software on GitHub</a> </div> </div> </div> </div> <div class="col-start-1 col-span-4"> <div class="flex flex-col pb-12 lg:pb-0 lg:ml-6 bg-primary lg:bg-page items-center">   </div> <div class="mx-6 my-6 flex flex-col gap-1"> <div class="flex flex-wrap mb-2 gap-3"> <p class="font-semibold">Development Team:</p> <a class="self-start border-b-2 transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="https://www.janelia.org/project-team/cellmap"> CellMap </a><a class="self-start border-b-2 transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="https://www.janelia.org/project-team/flyem"> FlyEM </a><a class="self-start border-b-2 transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="https://www.janelia.org/project-team/flylight"> FlyLight </a><a class="self-start border-b-2 transition ease-in duration-200 hover:bg-gray-100 hover:border-gray-600 dark:text-slate-300 dark:border-slate-500 dark:hover:bg-slate-800 dark:hover:border-slate-800 cursor-pointer" href="https://www.janelia.org/support-team/scientific-computing-software"> Scientific Computing Software </a> </div> <div class="flex flex-wrap mb-2 gap-3"> <p class="font-semibold">License:</p> BSD-3 Clause </div>   <div class="pt-6 flex flex-wrap gap-2"> <span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:var(--color-primary)"> CellMap </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:var(--color-primary)"> FlyEM </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:var(--color-primary)"> FlyLight </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:var(--color-primary)"> Scientific Computing Software </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#ab3f32"> Python </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#8b8883"> BSD-3 Clause </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#5084ac"> Command line application </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#ba5915"> Electron microscopy (EM) </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#7b0249"> HPC cluster </span><span class="bg-primary text-white px-2 py-1 rounded-md text-sm" style="background-color:#7b0249"> Local installation </span> </div> </div> <section class="col-span-4 row-start-3 ml-6 prose prose-lg lg:prose-xl dark:prose-invert dark:prose-headings:text-slate-300 prose-headings:font-heading prose-headings:leading-tighter prose-headings:tracking-tighter prose-headings:font-bold prose-a:text-primary dark:prose-a:text-blue-400 prose-img:rounded-md prose-img:shadow-lg"> <div class="my-6 md:mt-10"> <div class="border-t dark:border-slate-700"></div> </div> <!-- This is where any custom description of the project gets rendered --> <div><div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://zenodo.org/badge/latestdoi/232590929" rel="nofollow"><img src="https://camo.githubusercontent.com/e1e4f1988892eb05819053faa2ce788eb3582a777b7526c577565670e646633f/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3233323539303932392e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/232590929.svg" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">neuVid</h1><a id="user-content-neuvid" class="anchor" aria-label="Permalink: neuVid" href="#neuvid"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Summary</h2><a id="user-content-summary" class="anchor" aria-label="Permalink: Summary" href="#summary"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">These Python scripts generate simple anatomical videos in <a href="https://www.blender.org/" rel="nofollow">Blender</a>, following the conventions common in neuroscience research on the <em>Drosophila</em> fruit fly.  The input is a <a href="https://en.wikipedia.org/wiki/JSON" rel="nofollow">JSON</a> file giving a high-level description of anatomical elements (e.g., segmented neurons, regions of interest, synapses) and how they are animated (e.g., the camera frames on some neurons, then those neurons fade out while the camera orbits around them).  An experimental application can <a href="README.md#usage-with-natural-language-input">create the JSON from natural language using generative AI</a>.  Renderings of high quality can be done with a path-tracing renderer: Blender's Cycles, or the <a href="https://home.otoy.com/render/octane-render/" rel="nofollow">OTOY Octane renderer</a> (which requires a commercial license).  Here is a video scripted with <code>neuVid</code> and rendered with Octane (with titles added separately in <a href="https://www.apple.com/imovie/" rel="nofollow">iMovie</a>):</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=nu0b_tjCGxQ" rel="nofollow"><img src="https://camo.githubusercontent.com/70ff1b09804e9ec9080b72c494d0960a99d1a1605c015fc235a22ec445334090/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6e7530625f746a434778512f6d617872657364656661756c742e6a7067" alt="Fly Central Complex Circuitry" data-canonical-src="https://img.youtube.com/vi/nu0b_tjCGxQ/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto">See more examples in <a href="https://github.com/connectome-neuprint/neuVid/tree/master/documentation/gallery.md">the neuVid gallery</a>.</p>
<p dir="auto">These scripts also support volumetric data sets with no explicit segmentation. An example is the <a href="https://github.com/JaneliaSciComp/workstation/blob/master/docs/H5JFileFormat.md">H5J format</a> volumes in the <a href="https://splitgal4.janelia.org/cgi-bin/splitgal4.cgi" rel="nofollow">Janelia FlyLight Split-GAL4 driver collection</a>.  This kind of data is rendered with direct volume rendering by <a href="https://github.com/JaneliaSciComp/VVDViewer">VVDViewer</a>.</p>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with <code>neuPrint</code></h2><a id="user-content-usage-with-neuprint" class="anchor" aria-label="Permalink: Usage with neuPrint" href="#usage-with-neuprint"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The simplest way to start using <code>neuVid</code> is to import data from <a href="https://neuprint.janelia.org/" rel="nofollow"><code>neuPrint</code></a>, a web-based system for browsing the neurons and synapses in connectomes.  The set-up involves just two downloads, and the hands-on time can be as little as a few minutes, as demonstrated in the following tutorial video:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=bxbW0cumPPQ" rel="nofollow"><img src="https://camo.githubusercontent.com/a9b976cdd52e0fad70f60abea67a1b200c3368086a5d6181c1cba61214dc674c/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f627862573063756d5050512f6d617872657364656661756c742e6a7067" alt="Making Neuron Videos with neuPrint and neuVid" data-canonical-src="https://img.youtube.com/vi/bxbW0cumPPQ/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto">Here are the steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://www.blender.org/download/" rel="nofollow">Install Blender</a>.  These scripts will work with the latest version, and older versions back to 2.79.</p>
</li>
<li>
<p dir="auto">Open a terminal (shell) and clone this repository.</p>
</li>
<li>
<p dir="auto">Find some neurons (and synapses if desired) in <code>neuPrint</code>, and switch to <code>neuPrint</code>'s <a href="https://github.com/google/neuroglancer">Neuroglancer</a> tab.</p>
</li>
<li>
<p dir="auto">Press the "Copy view URL to clipboard" button (icon: two overlapping squares) at right side of the Neuroglancer header bar.</p>
</li>
<li>
<p dir="auto">Run the script to read the clipboard and output the JSON file that specifies the video.  In the following, <code>blender</code> is shorthand for the actual platform-specific path to the Blender executable: <code>/Applications/Blender.app/Contents/MacOS/Blender</code> on macOS; something like <code>/usr/local/blender/blender-3.4.1-linux-x64/blender</code> on Linux; something like <code>"C:\Program Files\Blender Foundation\Blender 3.4\blender.exe"</code> on Windows, with the quotes being necessary due to the spaces in the path.  Note also that with Windows PowerShell, the executable must be preceded by <code>&amp;</code>, as in <code>&amp; "C:\Program Files\Blender Foundation\Blender 3.4\blender.exe"</code>.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importNg.py -- -o ex1.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importNg.py -- -o ex1.json
</code></pre></div>
<p dir="auto">Note that on Windows, the path to the Blender executable may well contain spaces, as in <code>C:\Program Files\Blender Foundation\Blender 3.3\blender.exe</code>.  To run it in the command shell, put quotes around the path, as in <code>"C:\Program Files\Blender Foundation\Blender 3.3\blender.exe" --background --python ...</code>.</p>
<p dir="auto">Experimental option <code>-t</code> (or <code>--typesplit</code>): groups imported neurons by type, where available.</p>
</li>
<li>
<p dir="auto">Run the script to download meshes and create the basic Blender file (without animation) in the same directory as the JSON file.  This stage also creates directories for downloaded mesh files (<code>neuVidNeuronMeshes</code>,  <code>neuVidRoiMeshes</code>, <code>neuVidSynapseMeshes</code>) in the same directory as the JSON file.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importMeshes.py -- -i ex1.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importMeshes.py -- -i ex1.json
</code></pre></div>
</li>
<li>
<p dir="auto">Run the script to create a second Blender file with animation.  Adding animation is a separate step since it can take significantly less time than creating the basic Blender file, and may need to be done repeatedly as the animation specification is refined.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/addAnimation.py -- -i ex1.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/addAnimation.py -- -i ex1.json
</code></pre></div>
</li>
<li>
<p dir="auto">If desired, preview the animation by opening the second blender file (<code>ex1Anim.blend</code>) in a normal interactive (not background) Blender session.</p>
</li>
<li>
<p dir="auto">Run the script to render the animation with Blender's Cycles renderer.  This step takes the longest (tens of minutes on a modern desktop or laptop computer).  The rendered frames go to a subdirectory (<code>ex1-frames</code>) in the same directory as the JSON file.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/render.py -- -i ex1.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/render.py -- -i ex1.json
</code></pre></div>
</li>
<li>
<p dir="auto">Run the script to assemble the rendered frames into a video (named <code>ex1-frames/0001-N.avi</code>, where <code>N</code> is the number of rendered frames).</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex1.json"><pre class="notranslate"><code>blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex1.json
</code></pre></div>
</li>
</ol>
<p dir="auto">A second tutorial shows how to make more sophisticated videos with more camera motion:
<a href="https://www.youtube.com/watch?v=P3VbpETjCjY" rel="nofollow"><img src="https://camo.githubusercontent.com/d07b3a788ff2666236906c0827bc8aaa7acb774599184e25fc4b606193fa06f8/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f503356627045546a436a592f6d617872657364656661756c742e6a7067" alt="Camera Motion in Neuron Videos with neuPrint and neuVid" data-canonical-src="https://img.youtube.com/vi/P3VbpETjCjY/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with Neuroglancer</h2><a id="user-content-usage-with-neuroglancer" class="anchor" aria-label="Permalink: Usage with Neuroglancer" href="#usage-with-neuroglancer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Note how the <code>neuPrint</code> workflow involves <a href="https://github.com/google/neuroglancer">Neuroglancer</a>, a WebGL-based viewer for large data sets like those from connectomics.  Neuroglancer handles other data sets, and some of them can be imported into <code>neuVid</code>, too.  An example from neuroscience is the FAFB data set in its <a href="https://flywire.ai/" rel="nofollow">FlyWire</a> and <a href="https://fafb-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B4e-9%2C%22m%22%5D%2C%22y%22:%5B4e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%2C%22position%22:%5B109421.8984375%2C41044.6796875%2C5417%5D%2C%22crossSectionScale%22:2.1875%2C%22projectionOrientation%22:%5B-0.08939177542924881%2C-0.9848012924194336%2C-0.07470247149467468%2C0.12882165610790253%5D%2C%22projectionScale%22:27773.019357116023%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-fafb-data/fafb_v14/fafb_v14_orig%22%2C%22name%22:%22fafb_v14%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-fafb-data/fafb_v14/fafb_v14_clahe%22%2C%22name%22:%22fafb_v14_clahe%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://fafb-ffn1-20190805/segmentation%22%2C%22segments%22:%5B%22710435991%22%5D%2C%22name%22:%22fafb-ffn1-20190805%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://neuroglancer-20191211_fafbv14_buhmann2019_li20190805%22%2C%22tab%22:%22rendering%22%2C%22annotationColor%22:%22#cecd11%22%2C%22shader%22:%22#uicontrol%20vec3%20preColor%20color%28default=%5C%22blue%5C%22%29%5Cn#uicontrol%20vec3%20postColor%20color%28default=%5C%22red%5C%22%29%5Cn#uicontrol%20float%20scorethr%20slider%28min=0%2C%20max=1000%29%5Cn#uicontrol%20int%20showautapse%20slider%28min=0%2C%20max=1%29%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28defaultColor%28%29%29%3B%5Cn%20%20setEndpointMarkerColor%28%5Cn%20%20%20%20vec4%28preColor%2C%200.5%29%2C%5Cn%20%20%20%20vec4%28postColor%2C%200.5%29%29%3B%5Cn%20%20setEndpointMarkerSize%285.0%2C%205.0%29%3B%5Cn%20%20setLineWidth%282.0%29%3B%5Cn%20%20if%20%28int%28prop_autapse%28%29%29%20%3E%20showautapse%29%20discard%3B%5Cn%20%20if%20%28prop_score%28%29%3Cscorethr%29%20discard%3B%5Cn%7D%5Cn%5Cn%22%2C%22shaderControls%22:%7B%22scorethr%22:80%7D%2C%22linkedSegmentationLayer%22:%7B%22pre_segment%22:%22fafb-ffn1-20190805%22%2C%22post_segment%22:%22fafb-ffn1-20190805%22%7D%2C%22filterBySegmentation%22:%5B%22post_segment%22%2C%22pre_segment%22%5D%2C%22name%22:%22synapses_buhmann2019%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22n5://gs://fafb-v14-synaptic-clefts-heinrich-et-al-2018-n5/synapses_dt_reblocked%22%2C%22opacity%22:0.73%2C%22shader%22:%22void%20main%28%29%20%7BemitRGBA%28vec4%280.0%2C0.0%2C1.0%2CtoNormalized%28getDataValue%28%29%29%29%29%3B%7D%22%2C%22name%22:%22clefts_Heinrich_etal%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-fafb-data/elmr-data/FAFBNP.surf/mesh#type=mesh%22%2C%22segments%22:%5B%221%22%2C%2210%22%2C%2211%22%2C%2212%22%2C%2213%22%2C%2214%22%2C%2215%22%2C%2216%22%2C%2217%22%2C%2218%22%2C%2219%22%2C%222%22%2C%2220%22%2C%2221%22%2C%2222%22%2C%2223%22%2C%2224%22%2C%2225%22%2C%2226%22%2C%2227%22%2C%2228%22%2C%2229%22%2C%223%22%2C%2230%22%2C%2231%22%2C%2232%22%2C%2233%22%2C%2234%22%2C%2235%22%2C%2236%22%2C%2237%22%2C%2238%22%2C%2239%22%2C%224%22%2C%2240%22%2C%2241%22%2C%2242%22%2C%2243%22%2C%2244%22%2C%2245%22%2C%2246%22%2C%2247%22%2C%2248%22%2C%2249%22%2C%225%22%2C%2250%22%2C%2251%22%2C%2252%22%2C%2253%22%2C%2254%22%2C%2255%22%2C%2256%22%2C%2257%22%2C%2258%22%2C%2259%22%2C%226%22%2C%2260%22%2C%2261%22%2C%2262%22%2C%2263%22%2C%2264%22%2C%2265%22%2C%2266%22%2C%2267%22%2C%2268%22%2C%2269%22%2C%227%22%2C%2270%22%2C%2271%22%2C%2272%22%2C%2273%22%2C%2274%22%2C%2275%22%2C%228%22%2C%229%22%5D%2C%22name%22:%22neuropil-regions-surface%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22mesh%22%2C%22source%22:%22vtk://https://storage.googleapis.com/neuroglancer-fafb-data/elmr-data/FAFB.surf.vtk.gz%22%2C%22shader%22:%22void%20main%28%29%20%7BemitRGBA%28vec4%281.0%2C%200.0%2C%200.0%2C%200.5%29%29%3B%7D%22%2C%22name%22:%22neuropil-full-surface%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%7B%22url%22:%22precomputed://gs://fafb-ffn1-20190805/segmentation%22%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22precomputed://gs://fafb-ffn1-20190805/segmentation/skeletons_32nm%22%5D%2C%22selectedAlpha%22:0%2C%22segments%22:%5B%224613663523%22%5D%2C%22name%22:%22skeletons_32nm%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://fafb-ffn1/fafb-public-skeletons%22%2C%22name%22:%22public_skeletons%22%2C%22visible%22:false%7D%5D%2C%22showAxisLines%22:false%2C%22showSlices%22:false%2C%22layout%22:%22xy-3d%22%7D" rel="nofollow">FFN1</a> forms (<em>FAFB synapses are not yet supported</em>), and an example from cell biology is the <a href="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B1e-9%2C%22m%22%5D%2C%22y%22:%5B1e-9%2C%22m%22%5D%2C%22z%22:%5B1e-9%2C%22m%22%5D%7D%2C%22position%22:%5B24800.5%2C2000.5%2C19440.5%5D%2C%22crossSectionOrientation%22:%5B1%2C0%2C0%2C0%5D%2C%22crossSectionScale%22:50%2C%22projectionOrientation%22:%5B1%2C0%2C0%2C0%5D%2C%22projectionScale%22:65536%2C%22layers%22:%5B%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/er_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/er_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#0000ff%22%2C%22name%22:%22er_seg%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/endo_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/endo_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#ff00ff%22%2C%22name%22:%22endo_seg%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/golgi_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/golgi_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#00ffff%22%2C%22name%22:%22golgi_seg%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/mito_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/mito_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#008000%22%2C%22name%22:%22mito_seg%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/nucleus_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/nucleus_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#ff0000%22%2C%22name%22:%22nucleus_seg%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/pm_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/pm_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#ffa500%22%2C%22name%22:%22pm_seg%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/em/fibsem-uint8.precomputed%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%5D%2C%5B0%2C-1%2C0%2C4000%5D%2C%5B0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22x%22:%5B1e-9%2C%22m%22%5D%2C%22y%22:%5B1e-9%2C%22m%22%5D%2C%22z%22:%5B1e-9%2C%22m%22%5D%7D%7D%7D%2C%22tab%22:%22rendering%22%2C%22opacity%22:0.75%2C%22blend%22:%22additive%22%2C%22shader%22:%22#uicontrol%20invlerp%20normalized%28range=%5B55%2C%20240%5D%2C%20window=%5B0%2C%20255%5D%29%5Cn%20%20%20%20%20%20%20%20%20%20#uicontrol%20int%20invertColormap%20slider%28min=0%2C%20max=1%2C%20step=1%2C%20default=0%29%5Cn%20%20%20%20%20%20%20%20%20%20#uicontrol%20vec3%20color%20color%28default=%5C%22white%5C%22%29%5Cn%20%20%20%20%20%20%20%20%20%20float%20inverter%28float%20val%2C%20int%20invert%29%20%7Breturn%200.5%20+%20%28%282.0%20%2A%20%28-float%28invert%29%20+%200.5%29%29%20%2A%20%28val%20-%200.5%29%29%3B%7D%5Cn%20%20%20%20%20%20%20%20%20%20%20%20void%20main%28%29%20%7B%5Cn%20%20%20%20%20%20%20%20%20%20%20%20emitRGB%28color%20%2A%20inverter%28normalized%28%29%2C%20invertColormap%29%29%3B%5Cn%20%20%20%20%20%20%20%20%20%20%7D%22%2C%22name%22:%22fibsem-uint8%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%5B%22n5://s3://janelia-cosem-datasets/jrc_hela-3/jrc_hela-3.n5/labels/vesicle_seg%22%2C%22precomputed://s3://janelia-cosem-datasets/jrc_hela-3/neuroglancer/mesh/vesicle_seg%22%5D%2C%22tab%22:%22source%22%2C%22segmentDefaultColor%22:%22#ff0000%22%2C%22name%22:%22vesicle_seg%22%7D%5D%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22er_seg%22%7D%2C%22crossSectionBackgroundColor%22:%22#000000%22%2C%22layout%22:%224panel%22%7D" rel="nofollow">interphase HeLa cell</a> from the <a href="https://openorganelle.janelia.org/" rel="nofollow">OpenOrganelle</a> collection:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=3hVHbIRS48Q" rel="nofollow"><img src="https://camo.githubusercontent.com/186686a06070f6fb7d345f96076a49a506fb7f31d22db63b1349fbd13b00fa5a/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f33685648624952533438512f6d617872657364656661756c742e6a7067" alt="Interphase HeLa Cell" data-canonical-src="https://img.youtube.com/vi/3hVHbIRS48Q/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<p dir="auto">When importing from Neuroglancer outside <code>neuPrint</code>, some extra Python modules are needed.  These modules process the Neuroglancer <a href="https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/README.md">"precomputed"</a> format that stores <a href="https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/meshes.md#multi-resolution-mesh-format">multiresolution</a> and/or <a href="https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/sharded.md">sharded</a> meshes, converting the meshes to <a href="https://en.wikipedia.org/wiki/Wavefront_.obj_file" rel="nofollow">OBJ files</a> for futher processing in <code>neuVid</code>. A good way to manage the installation of these modules (so they do not interfere with other uses of Python) is to use <a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow">Miniconda</a>.  First, <a href="https://conda.io/projects/conda/en/stable/user-guide/install/index.html" rel="nofollow">install Miniconda itself</a>. (Note that there is an "arm64" version of Conda for Apple silicon, like the M1 chip, but the packages needed here do not yet work with it; use the legacy "x86_64" version of Conda instead.) Next, create an environment (named "neuVid-NG", for example) with the extra modules, like <a href="https://github.com/sdorkenw/MeshParty">MeshParty</a>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="conda create --name neuVid-NG python=3.9
conda activate neuVid-NG
pip install meshparty open3d"><pre class="notranslate"><code>conda create --name neuVid-NG python=3.9
conda activate neuVid-NG
pip install meshparty open3d
</code></pre></div>
<p dir="auto">Then follow these steps to use <code>neuVid</code>:</p>
<ol dir="auto">
<li>
<p dir="auto">Make sure Blender is installed, and this repository is cloned, as above.</p>
</li>
<li>
<p dir="auto">In Neuroglancer, make the desired segments visible (e.g., right-click on the layer tab to get the side bar, switch to the "Seg" tab, click the top check box to make all IDs visible).</p>
</li>
<li>
<p dir="auto">Click on the browser URL for Neuroglancer to select it, and copy it to the clipboard.  (It is a very long URL.)</p>
</li>
<li>
<p dir="auto">In a terminal (shell), activate the Conda environment to make the extra modules available:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" conda activate neuVid-NG"><pre class="notranslate"><code> conda activate neuVid-NG
</code></pre></div>
</li>
<li>
<p dir="auto">Run the script to convert the Neuroglancer URL in the clipboard into the JSON file that specifies the video.  Remember that <code>blender</code> is shorthand for the actual platform-specific path to the Blender executable, as described above.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importNg.py -- -o ex2.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importNg.py -- -o ex2.json
</code></pre></div>
</li>
<li>
<p dir="auto">Run the script that fetches the meshes from the Neuroglancer sources.  Note that this script runs with <code>python</code> directly instead of using Blender (which does not know about the extra Python modules).</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" python neuVid/neuVid/fetchMeshes.py -i ex2.json"><pre class="notranslate"><code> python neuVid/neuVid/fetchMeshes.py -i ex2.json
</code></pre></div>
</li>
<li>
<p dir="auto">Edit the <code>ex2.json</code> file to create the desired animation; see the <a href="https://github.com/connectome-neuprint/neuVid/tree/master/documentation">detailed <code>neuVid</code> documentation</a>.  That documentation discusses another approach to defining the animation, involving <a href="https://github.com/connectome-neuprint/neuVid/tree/master/documentation#neuroglancer">multiple Neuroglancer URLs that define key moments in the animation</a>.</p>
</li>
<li>
<p dir="auto">Run the remaining <code>neuVid</code> scripts as above:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importMeshes.py -- -i ex2.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex2.json
 blender --background --python neuVid/neuVid/render.py -- -i ex2.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex2.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importMeshes.py -- -i ex2.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex2.json
 blender --background --python neuVid/neuVid/render.py -- -i ex2.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex2.json
</code></pre></div>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage from Scratch</h2><a id="user-content-usage-from-scratch" class="anchor" aria-label="Permalink: Usage from Scratch" href="#usage-from-scratch"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you feel comfortable specifying the mesh sources and segment IDs by hand, as described in the <a href="https://github.com/connectome-neuprint/neuVid/tree/master/documentation#neuroglancer">detailed <code>neuVid</code> documentation</a>, then there is no need to start with <code>neuPrint</code> or Neuroglancer:</p>
<ol dir="auto">
<li>
<p dir="auto">Make sure Blender is installed, and this repository is cloned, as above.</p>
</li>
<li>
<p dir="auto">Create a JSON file (e.g., <code>ex3.json</code>) to specify the sources and animation.</p>
</li>
<li>
<p dir="auto">Run the last four <code>neuVid</code> script as above.  Remember that <code>blender</code> is shorthand for the actual platform-specific path to the Blender executable, as described above.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importMeshes.py -- -i ex3.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex3.json
 blender --background --python neuVid/neuVid/render.py -- -i ex3.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex3.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importMeshes.py -- -i ex3.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex3.json
 blender --background --python neuVid/neuVid/render.py -- -i ex3.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex3.json
</code></pre></div>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Advanced Usage with Synapses</h2><a id="user-content-advanced-usage-with-synapses" class="anchor" aria-label="Permalink: Advanced Usage with Synapses" href="#advanced-usage-with-synapses"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Neuroglancer synapse layers from <code>neuPrint</code> are imported as described above, but in some cases it is useful to have more control over the details of the synapses.  This control is provided by the <a href="https://github.com/connectome-neuprint/neuprint-python"><code>neuprint-python</code> module</a>.  As mentioned on its project page, <code>neuprint-python</code> can be installed with either <a href="https://docs.conda.io" rel="nofollow">Conda</a> or <a href="https://pypi.org/project/pip" rel="nofollow">Pip</a>.  Here are the steps:</p>
<ol dir="auto">
<li>
<p dir="auto">To use Conda, first <a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow">install Miniconda</a>.</p>
</li>
<li>
<p dir="auto">Create a new Conda environment and install <code>neuprint-python</code> from the <code>flyem-forge</code> channel.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" conda create --name neuVid-synapses
 conda activate neuVid-synapses
 conda install -c flyem-forge neuprint-python"><pre class="notranslate"><code> conda create --name neuVid-synapses
 conda activate neuVid-synapses
 conda install -c flyem-forge neuprint-python
</code></pre></div>
<p dir="auto">Or add <code>neuprint-python</code> to an existing Conda environment, like <code>neuVid-NG</code> created above.</p>
</li>
<li>
<p dir="auto">The <code>neuprint-python</code> code requires <code>neuPrint</code> credentials.  In a web browser, visit <a href="https://neuprint.janelia.org" rel="nofollow"><code>https://neuprint.janelia.org</code></a> and log on.  Click on the second button on the top right, and choose the "Account" menu item to show the "Account" page.</p>
</li>
<li>
<p dir="auto">Copy the three-or-so-line-long string from the "Auth Token:" section and use it to set the <code>NEUPRINT_APPLICATION_CREDENTIALS</code> environment variable in the shell where <code>neuVid</code> is to be run.  For a <code>bash</code> shell, use a command like the following:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" export NEUPRINT_APPLICATION_CREDENTIALS=&quot;eyJhbGci...xwRYI3dg&quot;"><pre class="notranslate"><code> export NEUPRINT_APPLICATION_CREDENTIALS="eyJhbGci...xwRYI3dg"
</code></pre></div>
</li>
<li>
<p dir="auto">Run the script to query the synapses.  This stage creates a directory for the synapse meshes, <code>neuVidSynapseMeshes</code>, in the same directory as the JSON file.   Note that this script runs with <code>python</code> directly instead of using Blender (which does not know about the <code>neuprint-python</code> module).</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" python neuVid/neuVid/buildSynapses.py -i ex4.json"><pre class="notranslate"><code> python neuVid/neuVid/buildSynapses.py -i ex4.json
</code></pre></div>
</li>
<li>
<p dir="auto">Run the last four <code>neuVid</code> scripts as above.  Remember that <code>blender</code> is shorthand for the actual platform-specific path to the Blender executable, as described above.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/importMeshes.py -- -i ex4.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex4.json
 blender --background --python neuVid/neuVid/render.py -- -i ex4.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex4.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/importMeshes.py -- -i ex4.json
 blender --background --python neuVid/neuVid/addAnimation.py -- -i ex4.json
 blender --background --python neuVid/neuVid/render.py -- -i ex4.json
 blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex4.json
</code></pre></div>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with SWC Files</h2><a id="user-content-usage-with-swc-files" class="anchor" aria-label="Permalink: Usage with SWC Files" href="#usage-with-swc-files"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Some projects, such as the <a href="https://www.janelia.org/project-team/mouselight" rel="nofollow">Janelia MouseLight project</a>, represent neurons in <a href="https://neuroinformatics.nl/swcPlus/" rel="nofollow">SWC format</a>. The extra step of converting SWC files to <a href="https://en.wikipedia.org/wiki/Wavefront_.obj_file" rel="nofollow">OBJ files</a> is handled automatically by <code>neuVid</code>.</p>
<ol dir="auto">
<li>
<p dir="auto">Create an input JSON file that mentions the SWC files, as in the examples from the <a href="documentation/README.md#swc-files">detailed documentation</a>.</p>
</li>
<li>
<p dir="auto">Run <code>importMeshes.py</code> as in the other examples. It will create OBJ files (in the <code>neuVidNeuronMeshes</code> directory, a sibling to the input JSON file) from the SWC files. See the <a href="documentation/README.md#swc-files">detailed documentation</a> for some options related to the size and resolution of the generated OBJ files.</p>
</li>
<li>
<p dir="auto">Use <code>buildSynapses.py</code>, <code>addAnimation.py</code>, <code>render.py</code>, <code>compLabels.py</code> (described below) and <code>assembleFrames.py</code> as in the other examples.</p>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Rendering on a Compute Cluster</h2><a id="user-content-rendering-on-a-compute-cluster" class="anchor" aria-label="Permalink: Rendering on a Compute Cluster" href="#rendering-on-a-compute-cluster"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Rendering can be performed on a compute cluster, a collection of computers shared between users to meet a facility's needs for high-performance computing (HPC). <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=started-quick-start-guide" rel="nofollow">IBM Spectrum LSF</a> is the HPC platform that <code>neuVid</code> assumes is available. Rendering on a cluster involves the following steps:</p>
<ol dir="auto">
<li>
<p dir="auto">Run <code>importNg.py</code>, <code>buildSynapses.py</code>, <code>importMeshes.py</code> and <code>addAnimation.py</code> as in the other examples.</p>
</li>
<li>
<p dir="auto">Make sure machines on the cluster can access the <code>.json</code> and <code>.blend</code> files, the Blender executable, and the directory for the final rendered frames.</p>
</li>
<li>
<p dir="auto">Open a shell (terminal) on the cluster's host machine for submitting jobs.</p>
</li>
<li>
<p dir="auto">The <code>neuVid</code> script for rendering on a cluster is <code>clusterRender.py</code>, and its arguments are almost identical to those for the standard <code>render.py</code> script.  Say standard rendering would be invoked as follows:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/render.py -- ..."><pre class="notranslate"><code> blender --background --python neuVid/neuVid/render.py -- ...
</code></pre></div>
<p dir="auto">Cluster rendering then would be invoked in this way:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/clusterRender.py -- -P account ..."><pre class="notranslate"><code> blender --background --python neuVid/neuVid/clusterRender.py -- -P account ...
</code></pre></div>
<p dir="auto">The new argument, <code>-P account</code>, specifies the account to be billed for the time on the cluster. Note that this use of the <code>clusterRendering.py</code> script is synchronous: the script does not finish until the cluster job comes off the "pending" queue and runs to completion.</p>
</li>
<li>
<p dir="auto">For additional options (e.g., to specify the cluster or the "slot" count), see the <a href="documentation/README.md#compute-cluster-usage">detailed documentation</a>.</p>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with Axes</h2><a id="user-content-usage-with-axes" class="anchor" aria-label="Permalink: Usage with Axes" href="#usage-with-axes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In some videos, it is helpful to include a small set of arrows in the corner indicating biological directions like anterior, posterior, dorsal, ventral.  Such axes can be added by running the <code>compAxes.py</code> script before assembling the final video with <code>assembleFrames.py</code>.  The <code>compAxes.py</code> script uses the camera from the Blender file produced by <code>addAnimation.py</code> so the axes match the camera motion.  This approach involves the following steps:</p>
<ol dir="auto">
<li>
<p dir="auto">Use <code>importNg.py</code>, <code>fetchMeshes.py</code>, <code>importMeshes.py</code>, <code>buildSynapses.py</code>, <code>addAnimation.py</code>, and <code>render.py</code> as in the other examples.</p>
</li>
<li>
<p dir="auto">Add the <code>axes</code> category as a sibling to <code>neurons</code>, <code>rois</code> and <code>synapses</code>, to specify the arrow's orientations and identifying labels (see the <a href="documentation/README.md#axes">detailed documentation</a>).</p>
</li>
<li>
<p dir="auto">If the axes need to disappear during parts of the video, add <code>fade</code> commands.</p>
</li>
<li>
<p dir="auto">Composite the axes onto the rendered frames:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/compAxes.py -- -i ex5.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/compAxes.py -- -i ex5.json
</code></pre></div>
<p dir="auto">The resulting frames will be in the directory (folder) <code>ex5-frames-axes</code>.</p>
</li>
<li>
<p dir="auto">If the details or timing of the axes needs revision, edit the <code>axes</code> category and <code>fade</code> commands and run only <code>compAxes.py</code> again.  Doing so is much faster than running <code>render.py</code>.</p>
</li>
<li>
<p dir="auto">Assemble the final video from these frames:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex5-frames-axes"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex5-frames-axes
</code></pre></div>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with Labels</h2><a id="user-content-usage-with-labels" class="anchor" aria-label="Permalink: Usage with Labels" href="#usage-with-labels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">One way to add textual labels and titles is to add them to the finished video with an interactive editing application like <a href="https://www.apple.com/imovie/" rel="nofollow">iMovie</a> or <a href="https://www.adobe.com/products/premiere.html" rel="nofollow">Premiere</a>.  Another way is to describe the labels in <code>neuVid</code>'s input JSON file and use the <code>compLabels.py</code> script to add the labels before assembling the final video with <code>assembleFrames.py</code>.  The latter approach makes it simpler to keep track of multiple labels, and to coordinate the timing of the labels with the timing of the animation.  This approach involves the following steps:</p>
<ol dir="auto">
<li>
<p dir="auto">Use <code>importNg.py</code>, <code>fetchMeshes.py</code>, <code>importMeshes.py</code>, <code>buildSynapses.py</code>, <code>addAnimation.py</code>, <code>render.py</code>, and <code>compAxes.py</code> as in the other examples.</p>
</li>
<li>
<p dir="auto">Define the labels and their timing with <code>label</code> commands in the JSON file (see the <a href="documentation/README.md#label">detailed documentation</a>).</p>
</li>
<li>
<p dir="auto">Composite the labels onto the rendered frames:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/compLabels.py -- -i ex6.json"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/compLabels.py -- -i ex6.json
</code></pre></div>
<p dir="auto">The resulting frames will be in the directory (folder) <code>ex6-frames-labeled</code>.  If axes have been added already, add the <code>-if</code> (<code>--inputFrames</code>) argument to indicate the directory with the input frames (produced by <code>compAxes.py</code>):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/compLabels.py -- -i ex6.json -if ex6-frames-axes"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/compLabels.py -- -i ex6.json -if ex6-frames-axes
</code></pre></div>
<p dir="auto">The resulting frames will be in the directory <code>ex6-frames-axes-labeled</code>.</p>
</li>
<li>
<p dir="auto">If the content or timing of the labels needs revision, edit the <code>label</code> commands and run only <code>compLabels.py</code> again.  Doing so is much faster than running <code>render.py</code>.</p>
</li>
<li>
<p dir="auto">Assemble the final video from these frames:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex6-frames-labeled"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex6-frames-labeled
</code></pre></div>
<p dir="auto">Or:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex6-frames-axes-labeled"><pre class="notranslate"><code> blender --background --python neuVid/neuVid/assembleFrames.py -- -i ex6-frames-axes-labeled
</code></pre></div>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with Natural Language Input</h2><a id="user-content-usage-with-natural-language-input" class="anchor" aria-label="Permalink: Usage with Natural Language Input" href="#usage-with-natural-language-input"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">An experimental component of <code>neuVid</code> takes a description of a video in natural language and translates it to JSON using
<a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence" rel="nofollow">generative AI</a>.  For now, at least, an <a href="https://console.anthropic.com/login" rel="nofollow">Anthropic API key</a> or <a href="https://platform.openai.com/signup" rel="nofollow">OpenAI API key</a> is required to use this component. Use the following steps:</p>
<ol dir="auto">
<li>
<p dir="auto">The <code>generate</code> desktop application launches the user interface for entering descriptions and generating JSON.  Install <code>generate</code> by downloading an executable from the <a href="https://github.com/connectome-neuprint/neuVid/releases">releases page of this repository</a>. <em>Chrome on macOS or Windows raises a dialog that incorrectly calls the compressed (.zip) file with the executable "suspicious" or "uncommon". To unblock and complete the donwload, press the right arrow on the dialog, then press the download button that appears. On Windows a "protected your PC" dialog may then appear; press "More info" and then "Run anyway".</em>  Safari on macOS and Chrome on Linux do not have these problems. After downloading, extract the executable from the .zip file: double-click on macOS, or right-click on Windows and choose "Extract All" or an item from the "7-Zip" menu, or use the <code>unzip</code> command on Linux. Move the executable to a standard place, like <code>/Applications</code> on macOS, or <code>C:\Program Files\newVid</code> on Windows, or <code>~/bin</code> on Linux.</p>
</li>
<li>
<p dir="auto">The first time <code>generate</code> is run, it prompts for the name of the large-language model (LLM) to use.  Enter an <a href="https://docs.anthropic.com/claude/docs/models-overview#claude-3-a-new-generation-of-ai" rel="nofollow">Anthropic model</a> (e.g., <code>claude-3-opus-20240229</code>, the best peforming model so far) or an <a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="nofollow">OpenAI model</a> (e.g., <code>gpt-4-0613</code>, which works better than <code>gpt-4-turbo-preview</code> for neuVid input). The model name is saved for use in futures sessions, and it can be changed using the "Settings/Model..." menu item.</p>
</li>
<li>
<p dir="auto">Then <code>generate</code> prompts for an API key, from either Anthropic or OpenAI based on the model just chosen. The entered key is saved for future sessions, and can be changed using the "Settings/API Key..." menu item.</p>
</li>
<li>
<p dir="auto">Type the description of the video in the lower text area, and press the "Generate" button.</p>
</li>
<li>
<p dir="auto">After some processing time (which could be a minute or so for longer descriptions), the generated JSON will appear in the upper text area.  Press the "Save..." button to save it to a file for use as input to the other <code>neuVid</code> scripts.</p>
</li>
</ol>
<p dir="auto">For more information, see the <a href="documentation/README.md#natural-language-input-and-generative-ai">detailed documentation</a>.</p>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Usage with VVDViewer</h2><a id="user-content-usage-with-vvdviewer" class="anchor" aria-label="Permalink: Usage with VVDViewer" href="#usage-with-vvdviewer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For volumetric data sets lacking a segmentation, use the following approach.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=OE9icXDM8q8" rel="nofollow"><img src="https://camo.githubusercontent.com/127f65b7add55297ccd81c92064e4e8d811e2c59f5ca1184a8966cc4b27c7c35/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f4f4539696358444d3871382f6d617872657364656661756c742e6a7067" alt="Representative Expression Patterns in the Drosophila Visual System" data-canonical-src="https://img.youtube.com/vi/OE9icXDM8q8/maxresdefault.jpg" style="max-width: 100%;"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install <a href="https://github.com/JaneliaSciComp/VVDViewer">VVDViewer</a>.  The simplest approach is to download an installer from the <a href="https://github.com/JaneliaSciComp/VVDViewer/releases">releases page</a>.</p>
</li>
<li>
<p dir="auto">Install <code>animateVvd</code> by downloading an executable from the <a href="https://github.com/connectome-neuprint/neuVid/releases">releases page of this repository</a>.  It will be a compressed (.zip) file, so extract it on macOS by double-clicking; or extract it on Windows by right-clicking and choosing "Extract All" or an item from the "7-Zip" menu; or extract it on Linux with the <code>unzip</code> command.  Move the executable to a standard place, like <code>/Applications</code> on macOS, or <code>C:\Program Files\newVid</code> on Windows, or <code>~/bin</code> on Linux.</p>
</li>
<li>
<p dir="auto">To get a head start on the animation, <code>animateVvd</code> can build a basic JSON from a directory of volumes in <a href="https://github.com/JaneliaSciComp/workstation/blob/master/docs/H5JFileFormat.md">H5J format</a>, say, <code>exampleVolumes</code>.  In a shell (terminal), run the following, where <code>animateVvd</code> is shorthand for the actual platform-specific path to the executable (something like <code>/Applications/animateVvd.app/Contents/MacOS/animateVvd</code> on macOS; or like <code>"C:\Program Files\neuVid\animateVvd.exe"</code> on Windows, where the quotes are significant since the path contains a space; or like <code>~/bin/animateVvd.bin</code> on Linux).</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" animateVvd -i exampleVolumes"><pre class="notranslate"><code> animateVvd -i exampleVolumes
</code></pre></div>
</li>
<li>
<p dir="auto">Edit <code>exampleVolumes.json</code> to add more animation commands.  See the <a href="documentation/README_VVD.md">detailed documentation</a>.</p>
</li>
<li>
<p dir="auto">Use <code>animateVvd</code> again, to convert <code>exampleVolumes.json</code> into a project file for VVDViewer, <code>exampleVolumes.vrp</code>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" animateVvd -i exampleVolumes.json"><pre class="notranslate"><code> animateVvd -i exampleVolumes.json
</code></pre></div>
</li>
<li>
<p dir="auto">Run VVDViewer, and press the "Open Project" button at the top to load <code>exampleVolumes.vrp</code>.</p>
</li>
<li>
<p dir="auto">Close all the VVDViewer panels except "Render: View 1" and "Record/Export" to make the rendered view as big as possible (as its size is the size of the final video).</p>
</li>
<li>
<p dir="auto">In the "Record/Export" panel, switch to the "Advanced" tab.</p>
</li>
<li>
<p dir="auto">Press the "Save..." button to render the video.  Do not press the "Play" button and then rewind back to the start before pressing "Save...", as doing so sometimes causes glitches in the saved video.</p>
</li>
</ol>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">History</h2><a id="user-content-history" class="anchor" aria-label="Permalink: History" href="#history"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">These scripts come from a collaboration at <a href="https://www.janelia.org/" rel="nofollow">HHMI's Janelia Research Campus</a> between the <a href="https://www.janelia.org/project-team/flyem" rel="nofollow">FlyEM project</a> and the <a href="https://www.janelia.org/support-team/scientific-computing-software" rel="nofollow">Scientific Computing Software group</a>.
The first use was for the <a href="https://www.janelia.org/project-team/flyem/hemibrain" rel="nofollow">hemibrain connectome release</a> in January, 2020.  Videos made with <code>neuVid</code> were the winner and second runner up for the <a href="https://drosophila-images.org/2021-2/" rel="nofollow">2021 Drosophila Image Award</a> from the <a href="https://genetics-gsa.org/" rel="nofollow">Genetics Society of America</a>.</p>
<div class="markdown-heading" dir="auto"><h2 class="heading-element" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" class="anchor" aria-label="Permalink: Acknowledgements" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://github.com/davidackerman">David Ackerman</a> contributed the first version of the code to fetch <a href="https://openorganelle.janelia.org/" rel="nofollow">OpenOrganelle</a> meshes.
<a href="https://dreherdesignstudio.com" rel="nofollow">Marisa Dreher</a> and <a href="https://github.com/floesche">Frank Loesche</a> helped improve the system's usability.</p>
</article></div></div> <!-- If no custom description in the Markdown file, the project's GitHub README file is used--> </section> </div> </div> </div>  <footer class="relative border-t border-gray-200 dark:border-slate-800 not-prose"> <div class="dark:bg-dark absolute inset-0 pointer-events-none" aria-hidden="true"></div> <div class="relative l mx-auto px-4 sm:px-6 dark:text-slate-300"> <div class="flex justify-between items-start my-6"> <div> <div class="mb-2"> <a class="inline-block font-bold text-xl" href="/pr-preview/pr-2/tmp/pr-preview/pr-2">ðŸ”¬ Janelia Software
</a> </div> <div class="flex flex-col gap-2 text-sm text-muted"> <a class="text-muted hover:text-gray-700 dark:text-gray-400 hover:underline transition duration-150 ease-in-out mr-2 rtl:mr-0 rtl:ml-2" href="https://www.hhmi.org/privacy-policy" target="_blank">Privacy Policy & Cookie Notice</a><a class="text-muted hover:text-gray-700 dark:text-gray-400 hover:underline transition duration-150 ease-in-out mr-2 rtl:mr-0 rtl:ml-2" href="https://github.com/JaneliaSciComp/ossi-website" target="_blank">This site is open source! Contribute on GitHub.</a> </div> </div> <div class="w-24 min-[400px]:w-32 md:w-44"> <a href="https://www.janelia.org/" target="_blank"> <img src="/pr-preview/pr-2/_astro/HHMI_Janelia_Logo-Black.YPERATnQ.png" alt="Logo for the Howard Hughes Medical Institute Janelia Research Campus" class="block dark:hidden"> <img src="/pr-preview/pr-2/_astro/HHMI_Janelia_Logo-White.EXklwjQ9.png" alt="Logo for the Howard Hughes Medical Institute Janelia Research Campus" class="hidden dark:block"> </a> </div> </div> <div>  </div> <div class="md:flex md:items-center md:justify-between pb-6"> <ul class="flex mb-4 md:order-1 md:ml-4 md:mb-0"> <li> <a class="text-muted dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm pr-5 md:pr-0 md:pl-5 inline-flex items-center" aria-label="Email" href="#"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5" data-icon="tabler:mail">  <symbol id="ai:tabler:mail"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M3 7a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/><path d="m3 7l9 6l9-6"/></g></symbol><use xlink:href="#ai:tabler:mail"></use>  </svg>  </a> </li><li> <a class="text-muted dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm pr-5 md:pr-0 md:pl-5 inline-flex items-center" aria-label="X" href="#"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5" data-icon="tabler:brand-x">  <symbol id="ai:tabler:brand-x"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m4 4l11.733 16H20L8.267 4zm0 16l6.768-6.768m2.46-2.46L20 4"/></symbol><use xlink:href="#ai:tabler:brand-x"></use>  </svg>  </a> </li><li> <a class="text-muted dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm pr-5 md:pr-0 md:pl-5 inline-flex items-center" aria-label="Github" href="https://github.com/JaneliaSciComp"> <svg width="1em" height="1em" viewBox="0 0 24 24" class="w-5 h-5" data-icon="tabler:brand-github">  <symbol id="ai:tabler:brand-github"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2c2.8-.3 5.5-1.4 5.5-6a4.6 4.6 0 0 0-1.3-3.2a4.2 4.2 0 0 0-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3 0 0 0-6.2 0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2 0 0 0-.1 3.2A4.6 4.6 0 0 0 4 9.5c0 4.6 2.7 5.7 5.5 6c-.6.6-.6 1.2-.5 2V21"/></symbol><use xlink:href="#ai:tabler:brand-github"></use>  </svg>  </a> </li> </ul> <div class="text-sm mr-4 dark:text-slate-400"> Â© 2024 Howard Hughes Medical Institute </div> </div> </div> </footer>  </body> </html>